[{"content":" Image Courtesy of SiFive, Inc.\nI recently picked up the SiFive HiFive 1 Rev B RISC-V microcontroller on Crowd Supply. According to the SiFive product page, the HiFive 1 Rev B is \u0026ldquo;an Arduino-compatible development board featuring the SiFive Freedom E310-G002 (FE310-G002) SoC plus a ESP32-SOLO-1 Wi-Fi and Bluetooth module\u0026rdquo;. I have seen examples for this board written in C, Rust, and TinyGo. Having not used Rust as much as I would like, I wanted to start there for development on this platform. Here is a thorough write-up of the process on Windows 11 via Windows Subsystem for Linux (February 2023).\nProcess This post assumes that you already have a properly configured (basic) Ubuntu 20.04 WSL install. Installation of Ubuntu on WSL is out of the scope of this post, although more information can be found here\nPrepare WSL for USB Devices Following the provided Microsoft Documentation, the first step to prepare WSL for USB is to run the following commands:\nsudo apt install linux-tools-5.4.0-77-generic hwdata sudo update-alternatives --install /usr/local/bin/usbip usbip /usr/lib/linux-tools/5.4.0-77-generic/usbip 20 Once complete, it\u0026rsquo;s important to ensure you have USB devices available on the host Windows 11 machine to passthrough. That can be accomplished with the following command (run in Powershell as Administrator):\nusbipd wsl list If an error is encountered stating that the usbipd cmdlet cannot be found (or anything similar), my first suggestion would be to update WSL (older versions of WSL didn\u0026rsquo;t have USB support). That can be done with two commands (again in an Administrator Powershell).\nwsl --update wsl --shutdown Once you are able to verify that you have devices available, we can continue to attaching the USB device to a WSL instance.\nIt should look something like this:\nPS C:\\WINDOWS\\system32\u0026gt; usbipd wsl list BUSID VID:PID DEVICE STATE 1-10 0b05:185c Realtek Bluetooth Adapter attached 1-12 1366:1061 JLink CDC UART Port (COM9), JLink CDC UART Port (COM8), B... Not attached Attaching a USB Device to WSL Our SiFive HiFive 1 Rev B uses a SEGGER JLink integration to communicate with the computer. We can see above that our board has a BUSID of 1-12. We will need to note that for a future step.\nNext, we want to identify the active distributions for WSL that we have on our computer. There is a set of WSL flags that can help us with this. (again in an Administrator Powershell)\nwsl --list --verbose For this post, we will be using the name Ubuntu-20.04.\nTo attach the USB device to our WSL instance, issue the following command (matching the two flags to the values we found above):\nusbipd wsl attach --busid 1-12 --distribution Ubuntu-20.04 Installing the JLink software on our WSL VM Some trickery is needed to download the JLink package from their website programatically. To accept the license agreement, download, and install the .deb package (explicitly for x86_64 architectures), run the following:\ncurl \u0026#39;https://www.segger.com/downloads/jlink/JLink_Linux_V784f_x86_64.deb\u0026#39; --data-raw \u0026#39;accept_license_agreement=accepted\u0026amp;submit=Download+software\u0026#39; --output JLink_Linux_x86_64.deb sudo dpkg -i JLink_Linux_x86_64.deb sudo apt-get -f install Installing Rust via Rustup rustup is the installer for the Rust Programming Language. For WSL, installation is managed with the following command:\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh You will need to select the default install for the purposes of this post. I ended up with this version: rustc 1.67.1\nInstalling cargo-generate To install cargo-generate, run the next command:\ncargo install cargo-generate Installing RISC-V Compilation Target To use our board, we will need a special compilation target. To install it:\nrustup target add riscv32imac-unknown-none-elf Installing SiFive GNU Toolchain for RISC-V To install the toolchain, we will need to download and unpack a file from the SiFive website. It\u0026rsquo;s no longer accessible (via their site) to my knowledge, but a direct link to their download server still works as of February 2023.\nThe last chmod may or may not be optional\u0026hellip; I didn\u0026rsquo;t bother to check.\ncurl https://static.dev.sifive.com/dev-tools/freedom-tools/v2020.12/riscv64-unknown-elf-toolchain-10.2.0-2020.12.8-x86_64-linux-ubuntu14.tar.gz --output /home/{your username}/riscv64-unknown-elf-toolchain-10.2.0-2020.12.8-x86_64-linux-ubuntu14.tar.gz tar -xvf /home/{your username}/riscv64-unknown-elf-toolchain-10.2.0-2020.12.8-x86_64-linux-ubuntu14.tar.gz chmod +x /home/{your username}/riscv64-unknown-elf-toolchain-10.2.0-2020.12.8-x86_64-linux-ubuntu14/bin/riscv64-unknown-elf-gdb Then, add the following to your $PATH in .bashrc:\nexport PATH=\u0026#34;$PATH:/home/{your username}/riscv64-unknown-elf-toolchain-10.2.0-2020.12.8-x86_64-linux-ubuntu14/bin\u0026#34; Starting the JLink GDB Server Starting the GDB server at this point is a fairly straightforward process. Just one command:\nJLinkGDBServer -device FE310 -if JTAG -speed 4000 -port 3333 -nogui Building and Deploying the Example Rust Code First, we need to instantiate the new application:\ncargo generate --git https://github.com/riscv-rust/riscv-rust-quickstart After naming the app, enter the new app\u0026rsquo;s directory:\ncd {app name} In the Cargo.toml file, I needed to make one adjustment. I bumped the dependency riscv-rt to 0.11.0 from 0.10.0. My final dependencies looked like this:\n[dependencies] embedded-hal = \u0026#34;0.2.7\u0026#34; hifive1 = { version = \u0026#34;0.10.0\u0026#34;, features = [\u0026#34;board-hifive1-revb\u0026#34;] } panic-halt = \u0026#34;0.2.0\u0026#34; riscv = \u0026#34;0.10.0\u0026#34; riscv-rt = \u0026#34;0.11.0\u0026#34; To run the LED Blink example, two final commands will be needed:\ncargo build --example leds_blink cargo run --example leds_blink Additionally, to view the serial console, you may try the following command:\nsudo screen /dev/ttyACM0 115200 Final Thoughts I\u0026rsquo;m excited to try writing some of my own Rust code for the SiFive HiFive 1 Rev B here in the coming weeks. I was admittedly late to the party (this board was released in April 2019), but I still see value in getting into the RISC-V architecture, especially in such an approachable form factor. If you\u0026rsquo;d like to pick one up for yourself, check out the Crowd Supply page!\nLinks for Future Research University of Kansas EECS 388 Lab #1 UK EECS 388 Schedule The Embedded Rust Book ","permalink":"https://michaelpeters.dev/posts/embedded-rust-programming-on-wsl/","summary":"Image Courtesy of SiFive, Inc.\nI recently picked up the SiFive HiFive 1 Rev B RISC-V microcontroller on Crowd Supply. According to the SiFive product page, the HiFive 1 Rev B is \u0026ldquo;an Arduino-compatible development board featuring the SiFive Freedom E310-G002 (FE310-G002) SoC plus a ESP32-SOLO-1 Wi-Fi and Bluetooth module\u0026rdquo;. I have seen examples for this board written in C, Rust, and TinyGo. Having not used Rust as much as I would like, I wanted to start there for development on this platform.","title":"Embedded Rust Programming on WSL (RISC-V)"},{"content":"Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?\nLoading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production. It worked well, almost too well. In the 10 minutes this change was live, it had written 4.5 million rows to BigQuery. I soon learned that we currently process nearly 10 times more traffic than is reported by our metrics dashboard. As it turned out, our metrics were counting incorrectly for years. Publishing that many logs to BigQuery did not make financial sense, even if the rows were partitioned by hour and only kept for a short period of time (6-12 hours). The error logs produced by this service are mainly used as a diagnostic tool in situations where our service is experiencing degraded performance.\nAfter determining that there wasn\u0026rsquo;t a significant amount of duplicates within the data, I was confident that the traffic was legitimate and our service processed too much volume to be stored in BigQuery.\nNOTE: Systems utilizing message queues often cannot perform \u0026ldquo;exactly-once\u0026rdquo; delivery, and therefore have a small but expected amount of duplicate messages.\nLuckily, we were also able to run some queries against the 10 minute slice of data to determine outlier error messages that could safely be dropped. Filtering out those error messages prior to insertion lead to about a 50% space savings, a great improvement.\nBack to the Drawing Board BigQuery was originally selected as our storage for these logs because of the powerful ability to rapidly query the data to make inferences and determine trends. It didn\u0026rsquo;t make good sense to stray away from the platform if our teams were already very familiar with it. We were beginning to run out of ideas for viable solutions, when I was reminded of the \u0026ldquo;External Tables\u0026rdquo; feature of BigQuery. I had previously utilized external tables on a small scale a few months back for another solution we were building. Using Google Cloud Storage (GCS) as our object store, we could batch and write these files on an interval and use wildcards in our query to build the final external table.\nHere\u0026rsquo;s an example of that query:\nCREATE EXTERNAL TABLE `project_id.dataset_name.table_name` OPTIONS ( format = \u0026#39;CSV\u0026#39;, uris = [\u0026#39;gs://error-logs/2022/06/29/2022-06-29-h19*\u0026#39;] ); Rewriting Logs Exporter for GCS Luckily, much of my original implementation was reusable. I quickly wrote a test application to simulate writing a gzip-compressed CSV file to GCS. With simple random test data there were no problems writing a file to GCS and loading it into BigQuery. Things were looking good. I added each piece of the test file into the main program, testing as I went. The end was in sight as piece-by-piece the solution came together. It was important to configure all of the intervals and constants as environment variables/flags so that they could easily be modified as needed.\nWith local testing showing promising results, it was time to deploy the changes.\nInitial Deployment As soon as the new deployment was live, it was clear that something was very wrong. The pod logs were indicating that the service account in production did not have the storage.objects.create permission. Yikes! In error, I had aquired the permissions for a different service account than the one that was being used in production. I think it\u0026rsquo;s time to update the documentation 🙂. Conveniently, when updating the IAM binding for testing, I used the role roles/storage.objectCreator. For this service though, we needed many other permissions, so I copied the set of permissions that weren\u0026rsquo;t already in the custom binding.\nresourcemanager.projects.list storage.objects.create storage.multipartUploads.create storage.multipartUploads.abort storage.multipartUploads.listParts As it turns out, the permission resourcemanager.projects.list is no longer valid (or at least no longer accepted by the Google API that Terraform interfaces with) which caused the service account to lose all permissions. How convenient! After determining why our logging queues were growing, I removed the erroneous permission and the queue quickly drained.\nData was flowing into GCS and seemed to be writing at the expected rate.\nExternal Table Woes The next day, I tried for the first time to load a CSV into BigQuery that contained production traffic. Quickly, I learned that the schema was unable to be auto-detected. At first glance, it was obvious why. Because we log response body in full, some responses contain whitespace commonly found in HTML pages such as return and tab characters. After fixing that issue, the schema reappeared as expected. The table still failed to be usable though. Unbeknownst to me at the time, Google BigQuery External Tables require only valid and printable UTF-8 code points for the STRING type. The BigQuery error was indicating that there were invalid characters present in the CSV files. Although difficult to find, the strings package contains a convienient helper function, strings.ToValidUTF8() that ensures a given string only consists of valid UTF-8 characters. As far as I knew, the problems were now solved. That was not the case. I still was receiving errors that read:\nError: Error detected while parsing row starting at position: {character number}. Error: Bad character (ASCII 0) encountered.\nI dug deeper into one of the raw CSV files and saw that there were some funny looking characters. Here\u0026rsquo;s an example of that: GIF89a\u0001�\u0001��\u0001�������!�\u0004\u0001�\u0001�,����\u0001�\u0001��\u0002\u0002L\u0001�; I didn\u0026rsquo;t realize it initially, but that sequence of characters is the headers of the GIF89a specification (of the popular .gif image format). A neat in-depth explanation can be found here, and helped show me how the GIF format works under the hood. When the GIF header is decoded as UTF-8 it becomes littered with Unicode control characters, most notably, NULL and START OF HEADING (0x00 and 0x01) respectively. These invisible control characters were the source of BigQuery\u0026rsquo;s vague error message. The unicode package in Go does have a function to detect non-printable runes (which includes all control characters, except space). To remedy the issue, a function was implemented to only allow printable characters to exist in a given string. This was applied to all strings in the logs, and provided the final solution to this strange course of investigation.\n// cleanString ensures characters in a string are printable (e.g not \u0026#34;control\u0026#34; characters) func cleanString(s string) string { return strings.Map(func(r rune) rune { if unicode.IsPrint(r) { return r } return -1 }, s) } Parting Thoughts Often, solutions in software may look easy at first glance. Many times, it evolves into something much more complicated, whether limited by performance, cost, or architecture. It is important to stay flexible and always think of possible alternatives, you never know when the situation may change. Persistance is also a valuable trait. There were many times over the course of this task where I didn\u0026rsquo;t see a successful outcome in sight, but I kept going. Eventually, an acceptable solution was found and I learned many valuable tips and tricks along the way.\n","permalink":"https://michaelpeters.dev/posts/leveraging-external-tables-in-google-big-query/","summary":"Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?\nLoading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production.","title":"Leveraging External Tables in Google BigQuery"},{"content":"Ansible for the Homelab It\u0026rsquo;s been a while since I last posted. Well, I\u0026rsquo;ve been working on something special. With my first server (an HP laptop with a broken screen) being nearly five years old, I was rapidly growing concerned about the health of it\u0026rsquo;s battery. The laptop has been plugged in and \u0026ldquo;floating\u0026rdquo; at 100% for the last 3-4 years. Given it\u0026rsquo;s previous life as a daily driven laptop (~7 years of total use), I assumed one day in the near future it would decide to become a spicy pillow. I wanted to avoid a house fire, so I picked up a Dell Optiplex 7060 Micro on eBay as it\u0026rsquo;s trusted replacement. It seemed like the perfect time to try something new with how I provision my servers, especially one that mostly remains in a static state.\nAnsible Overview Here\u0026rsquo;s a great overview of the Ansible tool from the creators:\nAnsible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems.\n— Ansible GitHub Repository\nAnsible is often used for provisioning server hardware alongside tools such as Vagrant and Terraform. In my homelab, I wanted to use this technology to my advantage. Automation will be key to preventing manual configuration down the road.\nOperating Sytem Choice Up to this point, I have manually configured all of my servers. I\u0026rsquo;ve tried a bit of everything, Raspbian, Ubuntu, Unraid, Arch, and even Amazon Linux. The newcomer to my current ecosystem is Rocky Linux. I was an early financial supporter of the Rocky Linux project because I wanted to support a no-frills enterprise focused Linux distribution. At this point, there\u0026rsquo;s a noticible lack of documentation when compared to other distributions like Ubuntu. Luckily, because it\u0026rsquo;s a derivative of RHEL, many of the commands for RHEL and CentOS are a drop-in replacement.\nImplementation Over the course of the last month, I\u0026rsquo;ve developed a handful of Ansible Playbooks to automate every step (except installation of an OS) of my new edge server. This server only handles NGINX and some local testing where a linux terminal is needed. I wanted to ensure that it\u0026rsquo;s easily repeatable in the event that I needed to start completely from scratch. It\u0026rsquo;s a two-part process. The first playbook is designed to setup the SSH configuration and ensure that SELinux is configured correctly. The second (which now utilizes key-pair authentication), is designed to follow my normal installation process step-by-step. The process goes as follows:\nSet up NTP Server connection to provide accurate time to the server (using Chrony) Download and install all of the relevant packages (for both CLI and system use) Download and configure Oh-My-ZSH with Antibody so that my terminal matches the rest of my computers Install Docker Install and configure Fail2Ban Setup the AWS CLI (which is used for Route53) Configure the Message Of The Day Install and configure Certbot to use Route53 for primary DNS Configure firewalld and SELinux to allow traffic on port 80 and 443 (http/s) Spawn the NGINX container with my reverse proxy configurations Gotchas Like many pieces of software, there is a right and wrong way of doing things. On first experience, I found it difficult to decouple the idea of running everything as a ansible.builtin.shell command. In Ansible, there are a plethora of modules that can be used to provide a more fluid experience. In particular, I utilized dnf, community.general.lastpass, template, and service the most. They fit my use cases quite well, and made the resulting YAML much smaller. These modules are great for abstracting away repeatable chunks of code.\nEndnote I plan on writing more on this subject in the future, or at least taking a deeper dive into how Ansible can be leveraged for the home user. In the meantime, check out Jeff Geerling. His blog and YouTube Channel are an indespensable wealth of information about Raspberry Pi\u0026rsquo;s, networking, homelab\u0026rsquo;s, and general tech content.\n","permalink":"https://michaelpeters.dev/posts/ansible-for-the-homelab/","summary":"Ansible for the Homelab It\u0026rsquo;s been a while since I last posted. Well, I\u0026rsquo;ve been working on something special. With my first server (an HP laptop with a broken screen) being nearly five years old, I was rapidly growing concerned about the health of it\u0026rsquo;s battery. The laptop has been plugged in and \u0026ldquo;floating\u0026rdquo; at 100% for the last 3-4 years. Given it\u0026rsquo;s previous life as a daily driven laptop (~7 years of total use), I assumed one day in the near future it would decide to become a spicy pillow.","title":"Ansible for the Homelab"},{"content":"ZSH! Below you can find the setup that I\u0026rsquo;ve found very comfortable while still maintaining a relatively minimal terminal experience. Confidential environment variables and aliases have been omitted to maintain basic operational security. This post will be edited every time a major revision is made.\nCustomizations of Note Plugin Manager: Antibody cat Alternative: bat Theme: lambda-gitster Current .zshrc Configuration export ZSH=\u0026#34;/Users/michaelpeters/.oh-my-zsh\u0026#34; export PATH=/usr/local/git/bin:$PATH ZSH_THEME=\u0026#34;lambda-gitster\u0026#34; DISABLE_UPDATE_PROMPT=\u0026#34;true\u0026#34; COMPLETION_WAITING_DOTS=\u0026#34;true\u0026#34; DISABLE_UNTRACKED_FILES_DIRTY=\u0026#34;true\u0026#34; source $ZSH/oh-my-zsh.sh source \u0026lt;(antibody init) antibody bundle \u0026lt; ~/.zshplugins export GPG_TTY=$(tty) export NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/nvm.sh\u0026#34; # This loads nvm [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/bash_completion\u0026#34; # This loads nvm bash_completion alias cat=\u0026#34;bat\u0026#34; alias speedUpGit=\u0026#34;git config --add oh-my-zsh.hide-status 1; git config --add oh-my-zsh.hide-dirty 1\u0026#34; alias gs=\u0026#34;git status\u0026#34; alias gcl=\u0026#34;git clone\u0026#34; alias gc=\u0026#34;git checkout\u0026#34; # GOAT: https://stackoverflow.com/questions/1057564/pretty-git-branch-graphs alias gl=\u0026#34;git log --graph --abbrev-commit --decorate --format=format:\u0026#39;%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)\u0026#39; --all\u0026#34; alias gcom=\u0026#34;git commit -S -m\u0026#34; alias ga=\u0026#34;git add\u0026#34; alias gpo=\u0026#34;git push origin\u0026#34; alias gp=\u0026#34;git pull\u0026#34; alias gd=\u0026#34;git diff\u0026#34; alias dcu=\u0026#34;docker compose up\u0026#34; alias dcub=\u0026#34;docker compose up --build\u0026#34; alias dcd=\u0026#34;docker compose down\u0026#34; alias dp=\u0026#34;docker push\u0026#34; alias mk=\u0026#34;minikube\u0026#34; timezsh() { shell=${1-$SHELL} for i in $(seq 1 10); do /usr/bin/time $shell -i -c exit; done } rl() { exec zsh } zshconfig() { nano ~/.zshrc } temp() { sudo powermetrics --samplers smc | grep -i \u0026#34;CPU die temperature\u0026#34; } ghm() { cd /Users/michaelpeters/go/src/github.com/michaelpeterswa } goland() { open -na \u0026#34;GoLand.app\u0026#34; --args \u0026#34;$@\u0026#34; } autoload -Uz promptinit; promptinit # The next line updates PATH for the Google Cloud SDK. if [ -f \u0026#39;/Users/michaelpeters/Downloads/google-cloud-sdk/path.zsh.inc\u0026#39; ]; then . \u0026#39;/Users/michaelpeters/Downloads/google-cloud-sdk/path.zsh.inc\u0026#39;; fi # The next line enables shell command completion for gcloud. if [ -f \u0026#39;/Users/michaelpeters/Downloads/google-cloud-sdk/completion.zsh.inc\u0026#39; ]; then . \u0026#39;/Users/michaelpeters/Downloads/google-cloud-sdk/completion.zsh.inc\u0026#39;; fi ","permalink":"https://michaelpeters.dev/posts/my-zsh-configuration/","summary":"ZSH! Below you can find the setup that I\u0026rsquo;ve found very comfortable while still maintaining a relatively minimal terminal experience. Confidential environment variables and aliases have been omitted to maintain basic operational security. This post will be edited every time a major revision is made.\nCustomizations of Note Plugin Manager: Antibody cat Alternative: bat Theme: lambda-gitster Current .zshrc Configuration export ZSH=\u0026#34;/Users/michaelpeters/.oh-my-zsh\u0026#34; export PATH=/usr/local/git/bin:$PATH ZSH_THEME=\u0026#34;lambda-gitster\u0026#34; DISABLE_UPDATE_PROMPT=\u0026#34;true\u0026#34; COMPLETION_WAITING_DOTS=\u0026#34;true\u0026#34; DISABLE_UNTRACKED_FILES_DIRTY=\u0026#34;true\u0026#34; source $ZSH/oh-my-zsh.sh source \u0026lt;(antibody init) antibody bundle \u0026lt; ~/.","title":"My ZSH Configuration"},{"content":"Introduction In a few cases while working on Go software, I have needed to generate a cryptographic hash (mainly SHA-256) from a string or JSON value. In all cases up to this point, I needed to convert the resulting []byte to a string representation for storage or viewing. In my investigation, I found two popular ways to approach this problem. I\u0026rsquo;ll describe them below as well as explore the performance implications of each choice.\nGenerating the Hash For all intents and purposes, this process will yield the same result for all common hashes such as MD5, SHA-1, SHA-256, and SHA-512. In my preliminary investigation, I only ran benchmarks on SHA-256. That benchmark data will be what we take a look at today. The process used is quite straightforward, and can be found here.\nOf note, the line shaBytes := sha256.Sum256([]byte(a)) is supplied with a string of 20 random characters in each test.\nMethod #1 (fmt.Sprintf) func MethodOne(a string) string { shaBytes := sha256.Sum256([]byte(a)) return fmt.Sprintf(\u0026#34;%x\u0026#34;, shaBytes) } This was the first method that I encountered in production code at work. Using the %x (hexadecimal) format specifier in conjunction with fmt.Sprintf() is a quick way to return the string representation of a hash. In my opinion, this method is more useful when printing to the console with fmt.Printf() as it requires one less line of code than the second method.\nMethod #2 (hex.EncodeToString) func MethodTwo(a string) string { shaBytes := sha256.Sum256([]byte(a)) return hex.EncodeToString(shaBytes[:]) } This was the method I settled upon to generate over 300,000 hashes-per-minute in multiple production services. When using this method it is important to remember that the EncodeToString() function requires a byte slice as it\u0026rsquo;s argument. To convert the byte array to a byte slice is as simple as using shaBytes[:], which can be seen above.\nPerformance Statistics Below is the direct output of the benchmarking process.\nλ sha_hex_string_bench main ✗ go test -run=XXX -bench=. -benchtime=100000x goos: darwin goarch: amd64 pkg: github.com/michaelpeterswa/sha_hex_string_bench cpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz BenchmarkShaHexStringWithHexLib-12 100000\t729.5 ns/op\t176 B/op\t4 allocs/op BenchmarkShaHexStringWithSprintf-12 100000\t966.3 ns/op\t176 B/op\t5 allocs/op BenchmarkRandStringBytes-12 100000\t382.2 ns/op\t48 B/op\t2 allocs/op PASS ok github.com/michaelpeterswa/sha_hex_string_bench\t0.336s And here is the simplified (and arguably more readable) table.\nBenchmarks iterations ns/op B/op allocs/op Method One (fmt.Sprintf) 100,000 966.3 176 5 Method Two (hex.EncodeToString) 100,000 729.5 176 4 Input String Generation 100,000 382.2 48 2 Conclusion As seen in the above data, hex.EncodeToString()is significantly faster. This is because the EncodeToString function does not rely on reflection internally like Sprintf does. It also has one less allocation to the heap for the same memory footprint, which is a benefit at scale. Although in most cases the added performance benefit wouldn\u0026rsquo;t matter, it\u0026rsquo;s important to take a step back and reflect on why certain design choices were made. Even small additions such as these SHA-256 hashes may negatively impact performance or increase cost in the long term. We want to avoid that! 😁\nFor more info about allocations and what they translate to under the hood check out this article: Understanding Allocations in Go\n","permalink":"https://michaelpeters.dev/posts/hashes-and-strings/","summary":"Introduction In a few cases while working on Go software, I have needed to generate a cryptographic hash (mainly SHA-256) from a string or JSON value. In all cases up to this point, I needed to convert the resulting []byte to a string representation for storage or viewing. In my investigation, I found two popular ways to approach this problem. I\u0026rsquo;ll describe them below as well as explore the performance implications of each choice.","title":"Hashes and Strings in Go"},{"content":"Preface (just a shoutout) Firstly, before digging into the real meat of this post, I\u0026rsquo;d like to thank GuildDarts on GitHub for creating the incredible Docker Folders plugin for Unraid (which greatly helped me clean up my Docker container list). I found this plugin after creating a new IPFS Docker container, which rounded out a cluttered ~30 containers running on my Dell R620. With it, I was able to make a new Web3 category to store my new related containers in. Now I have 6 or so container categories, and it is much more readable.\nIPFS Introduction As described on their website, IPFS is:\nA peer-to-peer hypermedia protocol designed to preserve and grow humanity\u0026rsquo;s knowledge by making the web upgradeable, resilient, and more open.\nIt\u0026rsquo;s a neat piece of software written in Go that is powering some of the early work in what the community is calling Web3. Essentially, IPFS is a distributed storage network, which allows access to data by any node on the network. By definition, it\u0026rsquo;s also decentralized. All the different pieces of data are spread out (distributed) across the network nodes.\nSo How Does It Work? I won\u0026rsquo;t pretend to understand the finer and more technical details of this project (yet), but their documentation (which coincedentally also uses IPFS to access) does an admirable job of summing up the three key points that make IPFS tick. They are as follows:\nUnique identification via content addressing Content linking via directed acyclic graphs (DAGs) Content discovery via distributed hash tables (DHTs) You can also read more here.\nWhat Am I Doing Here? I\u0026rsquo;ve recently become interested in exploring Blockchain, Crypto, and Web3 topics, and I wanted to explore the process of purchasing a domain from the Ethereum Name Service (which can be used to send Ethereum directly to me). In addition, using the ContentHash record, I wanted to host a simple HTML website from it. Hosting a file with an ENS (Ethereum Name Service) address can leverage IPFS to permanently (as long as it\u0026rsquo;s shared) host a website. What\u0026rsquo;s neat about this process is that it\u0026rsquo;s completely resistant to censorship, whether it be from parents, coworkers, employers, or even government agencies. Yeah that\u0026rsquo;s right, come at me NSA! I mean business. I\u0026rsquo;m surely already on one of your lists (NSA labels Linux Journal readers and Tor and Tails users as extremists).\nHosting IPFS Container The IPFS Desktop app conveniently provides the ability to access the IPFS network on your own computer. It does pose a problem though for persisting information on the network. IPFS coins the term pinning to describe the process of preventing the IPFS Node\u0026rsquo;s garbage collector from removing a specific file or files. Using pinning on the Desktop version of IPFS is great, but it has one major downfall. If the file hasn\u0026rsquo;t been shared yet, when your computer turns off, it will disappear from the IPFS network. This is where Docker comes in! I was able to provision an IPFS Docker Node following the instructions on their documentation, which can be found here.\nIPFS on Docker Hints The IPFS Docker container requires one port to be forwarded (4001 TCP/UDP) and also requires passing port 5001 to access the WebUI. Once started, you\u0026rsquo;ll need to access the WebUI at the following address http://\u0026lt;container-ip-address\u0026gt;:5001/webui as the root (without webui) will return a 404. Then, it\u0026rsquo;s incredibly important to follow the instructions provided on the WebUI to gain access to the IPFS API (by default it has very strict CORS requirements).\nUploading HTML Website The next step was to create and upload my simple HTML website to IPFS. In the Web3 mindset, I didn\u0026rsquo;t want to rely on any Web2 (conventional) services for my site. With that, the font is included (courtesy of Google Fonts) and the HTML/CSS files have no external scripts or dependencies. After starting my Docker IPFS Node, I was ready to begin the upload process. The easiest way I have found is to use the IPFS Desktop App alongside the Docker container to provide permanent pinning (without the use of a pinning service like Piñata).\nMy website has multiple files (a font, a stylesheet, and the HTML document), so the easiest way to upload this to IPFS is with a folder. I went through this process using the Windows IPFS Desktop application. It\u0026rsquo;s a pretty straightforward process, and gives you the option to select specific files within the folder to include/exclude in the final hash. Once the uploading process completes, you\u0026rsquo;ll be provided with.\nFor my Mac/Linux friends out there, the CLI (Command Line Interface) method to upload multiple files is also shown on the IPFS Documentation. It also allows for passing in an ignore file (so that you can ignore .git/ and other files). I plan to explore the CLI interface in the future as I don\u0026rsquo;t like to rely on Desktop applications when there\u0026rsquo;s a suitable alternative available.\nUpdating ContentHash For an Ethereum Name Service domain, the method of attributing some piece of distributed content is by setting it in the ContentHash record. Currently, the whole process is quite expensive (purchasing, registering, and updating an ENS domain) as the Ethereum network is quite congested. You can check the current Ethereum Gas fee here on ethgas.watch. Throughout the process, I paid between $60 and $80 USD in transaction fees. Uh oh!\nOnce I was ready, I grabbed the CID of my folder (which is now stored on IPFS) and logged into the Ethereum Name Service\u0026rsquo;s domain configuration page. From there, you are able to set the Content value for your domain, which in my case was an IPFS link. I then confirmed the transaction and paid the gas fee to have this ContentHash added to the Ethereum Blockchain. Unfortunately, at the time I didn\u0026rsquo;t know that it\u0026rsquo;s a good idea to use the IPNS (InterPlanetary Name Service) address, which allows future updates to the content. An example of that process can be found here. For the time being, my website\u0026rsquo;s content is unmodifiable and connected to my ENS Address.\nAccessing New IPFS Content There are multiple ways to access the website now! Two links that should work in normal browsers are:\nhttps://alpin3.eth.link https://alpin3.eth.limo The content is also accessible through IPFS at the link below:\nipfs://QmS8FUArChXPQDfxeVTFU4Bnn48m4knMqxQ1GjrWB5RjMK Final Notes This process has been a ton of fun to follow and learn about (except for those darn ⛽ fees) and I look forward to spending more time in the blockchain space in the future.\nHappy Hacking!\n","permalink":"https://michaelpeters.dev/posts/adventures-in-ipfs/","summary":"Preface (just a shoutout) Firstly, before digging into the real meat of this post, I\u0026rsquo;d like to thank GuildDarts on GitHub for creating the incredible Docker Folders plugin for Unraid (which greatly helped me clean up my Docker container list). I found this plugin after creating a new IPFS Docker container, which rounded out a cluttered ~30 containers running on my Dell R620. With it, I was able to make a new Web3 category to store my new related containers in.","title":"Adventures in IPFS"},{"content":"Issue This evening, I was struggling to access my new \u0026ldquo;7 Days To Die\u0026rdquo; server after provisioning a new Rocky Linux 8 VM to host it on. As a preface, I have plenty of experience hosting both bare-metal and VM game servers. For Steam games, my tool of choice is typically LinuxGSM by Daniel Gibbs. This is a fabulous piece of software that abstracts some of the nitty-gritty away (which definitely helps when running servers from multiple games).\nPort-Forwarding Normally, I would chalk a connection issue up to a misconfigured port forward. I quickly determined that another issue was at fault when I couldn\u0026rsquo;t access the server from the LAN. That is a telltale sign something else is at play.\nGoogle Searching After many dead ends, I stumbled upon this r/7daystodie thread where the OP noticed that there is a parameter in the settings file that prevents SteamNetworking from being enabled. I was dumbfounded that a default setting in the server config prevents networking/connection on the main platform of distribution.\nImportant Settings On line 15 of my configuration file (sdtdserver.xml), I removed SteamNetworking from the value field. This was based upon the suggestions from the above Reddit thread.\n\u0026lt;property name=\u0026#34;ServerDisabledNetworkProtocols\u0026#34; value=\u0026#34;SteamNetworking\u0026#34;/\u0026gt; \u0026lt;!-- Networking protocols that should not be used. Separated by comma. Possible values: LiteNetLib, SteamNetworking. Dedicated servers should disable SteamNetworking if there is no NAT router in between your users and the server or when port-forwarding is set up correctly --\u0026gt; Wrap-up I found that, when using a text-editor such as nano to edit the configuration file, the comment is obstructed which explains that SteamNetworking should be removed when \u0026ldquo;port-forwarding is set up correctly\u0026rdquo;. I would definitely say that this is user error, but from a developer side, it could definitely be more clear that this value needs to be modified before use (without relying on line-wrap). This value is easily missed and there is no emphasis or clarity that a server won\u0026rsquo;t be accessable unless you pay close attention to the configuration.\n","permalink":"https://michaelpeters.dev/posts/linuxgsm-7dtd-info/","summary":"Issue This evening, I was struggling to access my new \u0026ldquo;7 Days To Die\u0026rdquo; server after provisioning a new Rocky Linux 8 VM to host it on. As a preface, I have plenty of experience hosting both bare-metal and VM game servers. For Steam games, my tool of choice is typically LinuxGSM by Daniel Gibbs. This is a fabulous piece of software that abstracts some of the nitty-gritty away (which definitely helps when running servers from multiple games).","title":"LinuxGSM 7DaysToDie Tips"},{"content":"The Question? Should a software engineer use tabs or spaces when writing code?\nThe Answer? Use tabs, no really. In your favorite text-editor or IDE, simply set the tab length to 4 spaces.\nWhy? Tabs take only one byte, whereas the equivalent (4 spaces) takes four bytes. Realistically, this doesn\u0026rsquo;t matter nowadays with compression, minification, and large HDD/SSD storage mediums available. Tabs allow you to set the visual-width to your liking. Do you prefer 2 spaces or 8 spaces over the conventional 4 spaces? Change the default tab width and you\u0026rsquo;re off to the races. Go and Python (my current go-to languages) enforce consistency and significantly favor tabs over spaces. Example Program created with tabs!\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;errors\u0026#34; ) func main() { err := TabsAreSuperior(\u0026#34;tabs\u0026#34;) if err != nil { fmt.Println(err.Error()) } } func TabsAreSuperior(input string) error { switch input { case \u0026#34;tabs\u0026#34;: fmt.Println(\u0026#34;this is the way\u0026#34;) return nil case \u0026#34;spaces\u0026#34;: fmt.Println(\u0026#34;that\u0026#39;s not tabs\u0026#34;) return nil default: return errors.New(\u0026#34;please input either \u0026#39;tabs\u0026#39; or \u0026#39;spaces\u0026#39;\u0026#34;) } } ","permalink":"https://michaelpeters.dev/posts/spaces-vs-tabs/","summary":"The Question? Should a software engineer use tabs or spaces when writing code?\nThe Answer? Use tabs, no really. In your favorite text-editor or IDE, simply set the tab length to 4 spaces.\nWhy? Tabs take only one byte, whereas the equivalent (4 spaces) takes four bytes. Realistically, this doesn\u0026rsquo;t matter nowadays with compression, minification, and large HDD/SSD storage mediums available. Tabs allow you to set the visual-width to your liking.","title":"Spaces vs. Tabs"}]