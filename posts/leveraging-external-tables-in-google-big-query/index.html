<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Leveraging External Tables in Google BigQuery | michaelpeters</title><meta name=keywords content="blog,go,google,gcs,bigquery"><meta name=description content="Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?
Loading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production."><meta name=author content="michaelpeterswa"><link rel=canonical href=https://michaelpeters.dev/posts/leveraging-external-tables-in-google-big-query/><link crossorigin=anonymous href=/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css integrity="sha256-b2AFbUTT9+tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michaelpeters.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michaelpeters.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michaelpeters.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://michaelpeters.dev/apple-touch-icon.png><link rel=mask-icon href=https://michaelpeters.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.101.0"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Leveraging External Tables in Google BigQuery"><meta property="og:description" content="Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?
Loading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production."><meta property="og:type" content="article"><meta property="og:url" content="https://michaelpeters.dev/posts/leveraging-external-tables-in-google-big-query/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-29T13:09:15-07:00"><meta property="article:modified_time" content="2022-06-29T13:09:15-07:00"><meta property="og:site_name" content="michaelpeters"><meta name=twitter:card content="summary"><meta name=twitter:title content="Leveraging External Tables in Google BigQuery"><meta name=twitter:description content="Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?
Loading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michaelpeters.dev/posts/"},{"@type":"ListItem","position":2,"name":"Leveraging External Tables in Google BigQuery","item":"https://michaelpeters.dev/posts/leveraging-external-tables-in-google-big-query/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Leveraging External Tables in Google BigQuery","name":"Leveraging External Tables in Google BigQuery","description":"Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?\nLoading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production.","keywords":["blog","go","google","gcs","bigquery"],"articleBody":"Context Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing GET and POST requests to thousands of different endpoints. Simple enough, right?\nLoading Logs into BigQuery Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production. It worked well, almost too well. In the 10 minutes this change was live, it had written 4.5 million rows to BigQuery. I soon learned that we currently process nearly 10 times more traffic than is reported, which was in direct conflict with our metrics dashboard. As it turned out, our metrics were counting incorrectly for years. Publishing that many logs to BigQuery did not make financial sense, even if the rows were partitioned by hour and only kept for a short period of time (6-12 hours). The error logs produced by this service are mainly used as a diagnostic tool in situations where our service is experiencing degraded performance.\nAfter determining that there wasn‚Äôt a significant amount of duplicates within the data, I was confident that the traffic was legitimate and our service processed too much volume to be stored in BigQuery.\nNOTE: Systems utilizing message queues often cannot perform ‚Äúexactly-once‚Äù delivery, and therefore have a small but expected amount of duplicate messages.\nLuckily, we were also able to run some queries against the 10 minute slice of data to determine outlier error messages that could safely be dropped. Filtering out those error messages prior to insertion lead to about a 50% space savings, a great improvement.\nBack to the Drawing Board BigQuery was originally selected as our storage for these logs because of the powerful ability to rapidly query the data to make inferences and determine trends. It didn‚Äôt make good sense to stray away from the platform if our teams were already very familiar with it. We were beginning to run out of ideas for viable solutions, when I was reminded of the ‚ÄúExternal Tables‚Äù feature of BigQuery. I had previously utilized external tables on a small scale a few months back for another solution we were building. Using Google Cloud Storage (GCS) as our object store, we could batch and write these files on an interval and use wildcards in our query to build the final external table.\nHere‚Äôs an example of that query:\nCREATE EXTERNAL TABLE `project_id.dataset_name.table_name` OPTIONS ( format = 'CSV', uris = ['gs://error-logs/2022/06/29/2022-06-29-h19*'] ); Rewriting Logs Exporter for GCS Luckily, much of my original implementation was reusable. I quickly wrote a test application to simulate writing a gzip-compressed CSV file to GCS. With simple random test data there were no problems writing a file to GCS and loading it into BigQuery. Things were looking good. I added each piece of the test file into the main program, testing as I went. The end was in sight as piece-by-piece the solution came together. It was important to configure all of the intervals and constants as environment variables/flags so that they could easily be modified as needed.\nWith local testing showing promising results, it was time to deploy the changes.\nInitial Deployment As soon as the new deployment was live, it was clear that something was very wrong. The pod logs were indicating that the service account in production did not have the storage.objects.create permission. Yikes! In error, I had aquired the permissions for a different service account than the one that was being used in production. I think it‚Äôs time to update the documentation üôÇ. Conveniently, when updating the IAM binding for testing, I used the role roles/storage.objectCreator. For this service though, we needed many other permissions, so I copied the set of permissions that weren‚Äôt already in the custom binding.\nresourcemanager.projects.list storage.objects.create storage.multipartUploads.create storage.multipartUploads.abort storage.multipartUploads.listParts As it turns out, the permission resourcemanager.projects.list is no longer valid (or at least no longer accepted by the Google API that Terraform interfaces with) which caused the service account to lose all permissions. How convenient! After determining why our logging queues were growing, I removed the erroneous permission and the queue quickly drained.\nData was flowing into GCS and seemed to be writing at the expected rate.\nExternal Table Woes The next day, I tried for the first time to load a CSV into BigQuery that contained production traffic. Quickly, I learned that the schema was unable to be auto-detected. At first glance, it was obvious why. Because we log response body in full, some responses contain whitespace commonly found in HTML pages such as return and tab characters. After fixing that issue, the schema reappeared as expected. The table still failed to be usable though. Unbeknownst to me at the time, Google BigQuery External Tables require only valid and printable UTF-8 code points for the STRING type. The BigQuery error was indicating that there were invalid characters present in the CSV files. Although difficult to find, the strings package contains a convienient helper function, strings.ToValidUTF8() that ensures a given string only consists of valid UTF-8 characters. As far as I knew, the problems were now solved. That was not the case. I still was receiving errors that read:\nError: Error detected while parsing row starting at position: {character number}. Error: Bad character (ASCII 0) encountered.\nI dug deeper into one of the raw CSV files and saw that there were some funny looking characters. Here‚Äôs an example of that: GIF89a\u0001ÔøΩ\u0001ÔøΩÔøΩ\u0001ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ!ÔøΩ\u0004\u0001ÔøΩ\u0001ÔøΩ,ÔøΩÔøΩÔøΩÔøΩ\u0001ÔøΩ\u0001ÔøΩÔøΩ\u0002\u0002L\u0001ÔøΩ; I didn‚Äôt realize it initially, but that sequence of characters is the headers of the GIF89a specification (of the popular .gif image format). A neat in-depth explanation can be found here, and helped show me how the GIF format works under the hood. When the GIF header is decoded as UTF-8 it becomes littered with Unicode control characters, most notably, NULL and START OF HEADING (0x00 and 0x01) respectively. These invisible control characters were the source of BigQuery‚Äôs vague error message. The unicode package in Go does have a function to detect non-printable characters in a string (which includes all control characters, except space). To remedy the issue, a function was implemented to only allow printable characters to exist in a given string. This was applied to all strings in the logs, and provided the final solution to this strange course of investigation.\n// cleanString ensures characters in a string are printable (e.g not \"control\" characters) func cleanString(s string) string { return strings.Map(func(r rune) rune { if unicode.IsPrint(r) { return r } return -1 }, s) } Parting Thoughts Often, solutions in software may look easy at first glance. Many times, it evolves into something much more complicated, whether limited by performance, cost, or architecture. It is important to stay flexible and always think of possible alternatives, you never know when the situation may change. Persistance is also a valuable trait. There were many times over the course of this task where I didn‚Äôt see a successful outcome in sight, but I kept going. Eventually, an acceptable solution was found and I learned many valuable tips and tricks along the way.\n","wordCount":"1187","inLanguage":"en","datePublished":"2022-06-29T13:09:15-07:00","dateModified":"2022-06-29T13:09:15-07:00","author":{"@type":"Person","name":"michaelpeterswa"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michaelpeters.dev/posts/leveraging-external-tables-in-google-big-query/"},"publisher":{"@type":"Organization","name":"michaelpeters","logo":{"@type":"ImageObject","url":"https://michaelpeters.dev/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://michaelpeters.dev/ accesskey=h title="michaelpeters (Alt + H)">michaelpeters</a>
<span class=logo-switches></span></div><ul id=menu><li><a href=https://michaelpeters.dev/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michaelpeters.dev/>Home</a>&nbsp;¬ª&nbsp;<a href=https://michaelpeters.dev/posts/>Posts</a></div><h1 class=post-title>Leveraging External Tables in Google BigQuery</h1><div class=post-meta><span title='2022-06-29 13:09:15 -0700 -0700'>June 29, 2022</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;michaelpeterswa&nbsp;|&nbsp;<a href=https://github.com/michaelpeterswa/blog/blob/main/content/posts/leveraging-external-tables-in-google-big-query.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=context>Context<a hidden class=anchor aria-hidden=true href=#context>#</a></h2><p>Recently, I was tasked with logging error messages, request data, and the response body of failed HTTP requests to BigQuery for a service that makes outgoing <code>GET</code> and <code>POST</code> requests to thousands of different endpoints. Simple enough, right?</p><h2 id=loading-logs-into-bigquery>Loading Logs into BigQuery<a hidden class=anchor aria-hidden=true href=#loading-logs-into-bigquery>#</a></h2><p>Thankfully, the project already contained a legacy BigQuery streaming implementation which I used as my batch writer. After a thorough review of the changes, it was deployed to production. It worked well, almost too well. In the 10 minutes this change was live, it had written 4.5 million rows to BigQuery. I soon learned that we currently process nearly 10 times more traffic than is reported, which was in direct conflict with our metrics dashboard. As it turned out, our metrics were counting incorrectly for years. Publishing that many logs to BigQuery did not make financial sense, even if the rows were partitioned by hour and only kept for a short period of time (6-12 hours). The error logs produced by this service are mainly used as a diagnostic tool in situations where our service is experiencing degraded performance.</p><p>After determining that there wasn&rsquo;t a significant amount of duplicates within the data, I was confident that the traffic was legitimate and our service processed too much volume to be stored in BigQuery.</p><blockquote><p>NOTE: Systems utilizing message queues often cannot perform &ldquo;exactly-once&rdquo; delivery, and therefore have a small but expected amount of duplicate messages.</p></blockquote><p>Luckily, we were also able to run some queries against the 10 minute slice of data to determine outlier error messages that could safely be dropped. Filtering out those error messages prior to insertion lead to about a 50% space savings, a great improvement.</p><h2 id=back-to-the-drawing-board>Back to the Drawing Board<a hidden class=anchor aria-hidden=true href=#back-to-the-drawing-board>#</a></h2><p>BigQuery was originally selected as our storage for these logs because of the powerful ability to rapidly query the data to make inferences and determine trends. It didn&rsquo;t make good sense to stray away from the platform if our teams were already very familiar with it. We were beginning to run out of ideas for viable solutions, when I was reminded of the &ldquo;<a href=https://cloud.google.com/bigquery/docs/external-tables>External Tables</a>&rdquo; feature of BigQuery. I had previously utilized external tables on a small scale a few months back for another solution we were building. Using Google Cloud Storage (GCS) as our object store, we could batch and write these files on an interval and use wildcards in our query to build the final external table.</p><p>Here&rsquo;s an example of that query:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-SQL data-lang=SQL><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>EXTERNAL</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>project_id.dataset_name.<span style=color:#66d9ef>table_name</span><span style=color:#f92672>`</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>OPTIONS</span> (
</span></span><span style=display:flex><span>	format <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;CSV&#39;</span>,
</span></span><span style=display:flex><span>	uris <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;gs://error-logs/2022/06/29/2022-06-29-h19*&#39;</span>]
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><h2 id=rewriting-logs-exporter-for-gcs>Rewriting Logs Exporter for GCS<a hidden class=anchor aria-hidden=true href=#rewriting-logs-exporter-for-gcs>#</a></h2><p>Luckily, much of my original implementation was reusable. I quickly wrote a test application to simulate writing a gzip-compressed CSV file to GCS. With simple random test data there were no problems writing a file to GCS and loading it into BigQuery. Things were looking good. I added each piece of the test file into the main program, testing as I went. The end was in sight as piece-by-piece the solution came together. It was important to configure all of the intervals and constants as environment variables/flags so that they could easily be modified as needed.</p><p>With local testing showing promising results, it was time to deploy the changes.</p><h2 id=initial-deployment>Initial Deployment<a hidden class=anchor aria-hidden=true href=#initial-deployment>#</a></h2><p>As soon as the new deployment was live, it was clear that something was very wrong. The pod logs were indicating that the service account in production did not have the <code>storage.objects.create</code> permission. Yikes! In error, I had aquired the permissions for a different service account than the one that was being used in production. I think it&rsquo;s time to update the documentation üôÇ. Conveniently, when updating the IAM binding for testing, I used the role <code>roles/storage.objectCreator</code>. For this service though, we needed many other permissions, so I copied the set of permissions that weren&rsquo;t already in the custom binding.</p><pre tabindex=0><code>resourcemanager.projects.list
storage.objects.create
storage.multipartUploads.create
storage.multipartUploads.abort
storage.multipartUploads.listParts
</code></pre><p>As it turns out, the permission <code>resourcemanager.projects.list</code> is no longer valid (or at least no longer accepted by the Google API that Terraform interfaces with) which caused the service account to lose all permissions. How convenient! After determining why our logging queues were growing, I removed the erroneous permission and the queue quickly drained.</p><p>Data was flowing into GCS and seemed to be writing at the expected rate.</p><h2 id=external-table-woes>External Table Woes<a hidden class=anchor aria-hidden=true href=#external-table-woes>#</a></h2><p>The next day, I tried for the first time to load a CSV into BigQuery that contained production traffic. Quickly, I learned that the schema was unable to be auto-detected. At first glance, it was obvious why. Because we log response body in full, some responses contain whitespace commonly found in HTML pages such as return and tab characters. After fixing that issue, the schema reappeared as expected. The table still failed to be usable though. Unbeknownst to me at the time, Google BigQuery External Tables require only valid and printable UTF-8 code points for the STRING type. The BigQuery error was indicating that there were invalid characters present in the CSV files. Although difficult to find, the <code>strings</code> package contains a convienient helper function, <a href=https://pkg.go.dev/strings#ToValidUTF8>strings.ToValidUTF8()</a> that ensures a given string only consists of valid UTF-8 characters. As far as I knew, the problems were now solved. That was not the case. I still was receiving errors that read:</p><blockquote><p>Error: Error detected while parsing row starting at position: {character number}. Error: Bad character (ASCII 0) encountered.</p></blockquote><p>I dug deeper into one of the raw CSV files and saw that there were some funny looking characters. Here&rsquo;s an example of that: <a href="https://apps.timwhitlock.info/unicode/inspect?s=GIF89a%01%00%01%00%EF%BF%BD%01%00%EF%BF%BD%EF%BF%BD%EF%BF%BD%00%00%00%21%EF%BF%BD%04%01%00%01%00%2C%00%00%00%00%01%00%01%00%00%02%02L%01%00%3B">GIF89aÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ!ÔøΩÔøΩÔøΩ,ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩLÔøΩ;</a> I didn&rsquo;t realize it initially, but that sequence of characters is the headers of the <code>GIF89a</code> specification (of the popular <code>.gif</code> image format). A neat in-depth explanation can be found <a href=https://www.matthewflickinger.com/lab/whatsinagif/bits_and_bytes.asp>here</a>, and helped show me how the GIF format works under the hood. When the GIF header is decoded as UTF-8 it becomes littered with Unicode control characters, most notably, <code>NULL</code> and <code>START OF HEADING</code> (0x00 and 0x01) respectively. These invisible control characters were the source of BigQuery&rsquo;s vague error message. The <code>unicode</code> package in Go does have a function to detect non-printable characters in a string (which includes all control characters, except space). To remedy the issue, a function was implemented to only allow printable characters to exist in a given string. This was applied to all strings in the logs, and provided the final solution to this strange course of investigation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>// cleanString ensures characters in a string are printable (e.g not &#34;control&#34; characters)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cleanString</span>(<span style=color:#a6e22e>s</span> <span style=color:#66d9ef>string</span>) <span style=color:#66d9ef>string</span> {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>strings</span>.<span style=color:#a6e22e>Map</span>(<span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>r</span> <span style=color:#66d9ef>rune</span>) <span style=color:#66d9ef>rune</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> <span style=color:#a6e22e>unicode</span>.<span style=color:#a6e22e>IsPrint</span>(<span style=color:#a6e22e>r</span>) {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>r</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>	}, <span style=color:#a6e22e>s</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=parting-thoughts>Parting Thoughts<a hidden class=anchor aria-hidden=true href=#parting-thoughts>#</a></h2><p>Often, solutions in software may look easy at first glance. Many times, it evolves into something much more complicated, whether limited by performance, cost, or architecture. It is important to stay flexible and always think of possible alternatives, you never know when the situation may change. Persistance is also a valuable trait. There were many times over the course of this task where I didn&rsquo;t see a successful outcome in sight, but I kept going. Eventually, an acceptable solution was found and I learned many valuable tips and tricks along the way.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://michaelpeters.dev/tags/blog/>blog</a></li><li><a href=https://michaelpeters.dev/tags/go/>go</a></li><li><a href=https://michaelpeters.dev/tags/google/>google</a></li><li><a href=https://michaelpeters.dev/tags/gcs/>gcs</a></li><li><a href=https://michaelpeters.dev/tags/bigquery/>bigquery</a></li></ul><nav class=paginav><a class=next href=https://michaelpeters.dev/posts/ansible-for-the-homelab/><span class=title>Next Page ¬ª</span><br><span>Ansible for the Homelab</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Leveraging External Tables in Google BigQuery on twitter" href="https://twitter.com/intent/tweet/?text=Leveraging%20External%20Tables%20in%20Google%20BigQuery&url=https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f&hashtags=blog%2cgo%2cgoogle%2cgcs%2cbigquery"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Leveraging External Tables in Google BigQuery on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f&title=Leveraging%20External%20Tables%20in%20Google%20BigQuery&summary=Leveraging%20External%20Tables%20in%20Google%20BigQuery&source=https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Leveraging External Tables in Google BigQuery on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f&title=Leveraging%20External%20Tables%20in%20Google%20BigQuery"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Leveraging External Tables in Google BigQuery on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Leveraging External Tables in Google BigQuery on whatsapp" href="https://api.whatsapp.com/send?text=Leveraging%20External%20Tables%20in%20Google%20BigQuery%20-%20https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Leveraging External Tables in Google BigQuery on telegram" href="https://telegram.me/share/url?text=Leveraging%20External%20Tables%20in%20Google%20BigQuery&url=https%3a%2f%2fmichaelpeters.dev%2fposts%2fleveraging-external-tables-in-google-big-query%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://michaelpeters.dev/>michaelpeters</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>